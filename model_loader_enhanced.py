#!/usr/bin/env python3
"""
Enhanced model loading system for Stable Audio API with finetune support
Handles both standard SAOS and custom finetunes with smart memory management
NOW SUPPORTS AUTO-DETECTION OF VERSIONED CONFIGS FROM CHECKPOINT NAMES
FIXED: Proper config priority (HF repo first) and EMA weight handling
"""

import os
import json
import re
import threading
import gc
import torch
from contextlib import contextmanager
from huggingface_hub import login, hf_hub_download
from stable_audio_tools import get_pretrained_model
from stable_audio_tools.models import create_model_from_config
from stable_audio_tools.models.utils import load_ckpt_state_dict, copy_state_dict

class ModelManager:
    """Smart model cache with memory management for multiple model support"""
    
    def __init__(self, max_models=2):
        self.model_cache = {}
        self.model_lock = threading.Lock()
        self.max_models = max_models
        self.model_usage_order = []  # Track usage for LRU eviction
        
    @contextmanager
    def resource_cleanup(self):
        """Context manager to ensure proper cleanup of GPU resources."""
        try:
            yield
        finally:
            if torch.cuda.is_available():
                torch.cuda.synchronize()
                torch.cuda.empty_cache()
            gc.collect()
    
    def _evict_lru_model(self):
        """Evict least recently used model to free memory"""
        if len(self.model_cache) < self.max_models:
            return
        
        # Find least recently used model
        for model_key in self.model_usage_order:
            if model_key in self.model_cache:
                print(f"ðŸ§¹ Evicting LRU model: {model_key}")
                del self.model_cache[model_key]
                self.model_usage_order.remove(model_key)
                
                # Force cleanup
                with self.resource_cleanup():
                    pass
                break
    
    def _update_usage(self, model_key):
        """Update model usage order for LRU tracking"""
        if model_key in self.model_usage_order:
            self.model_usage_order.remove(model_key)
        self.model_usage_order.append(model_key)
    
    def _detect_config_from_checkpoint(self, checkpoint_name, finetune_repo):
        """
        Auto-detect the matching config file from checkpoint filename.
        
        Supports multiple patterns:
        - acid_v4_saos_e22_s16896.ckpt â†’ acid_v4_saos_e22_s16896_model_config.json (v4 style)
        - acid_v1_saos_e21_s33000.ckpt â†’ acid_v1_base_model_config.json (v1-v3 style)
        - acid_v2_*.ckpt â†’ acid_v2_config.json (flexible fallback)
        - am_saos1_e18_s4800.ckpt â†’ fallback to repo/base config
        
        FIXED: Now prioritizes HF repo over local file
        
        Returns: config_path (local or downloaded), or None if should use base_repo
        """
        print(f"ðŸ” Detecting config for checkpoint: {checkpoint_name}")
        
        # Extract version prefix for pattern matching
        version_match = re.match(r'(acid_v\d+)_', checkpoint_name)
        
        if version_match:
            version_prefix = version_match.group(1)
            print(f"   âœ… Detected version prefix: {version_prefix}")
            
            # Try multiple naming patterns in order of specificity
            checkpoint_base = checkpoint_name.replace('.ckpt', '')
            
            patterns_to_try = [
                # Pattern 1: Exact checkpoint name + _model_config.json (for v4)
                f"{checkpoint_base}_model_config.json",
                
                # Pattern 2: Version prefix + _base_model_config.json (for v1-v3)
                f"{version_prefix}_base_model_config.json",
                
                # Pattern 3: Version prefix + _model_config.json (variant)
                f"{version_prefix}_model_config.json",
                
                # Pattern 4: Just version + _config.json (most flexible)
                f"{version_prefix}_config.json",
            ]
            
            for i, pattern in enumerate(patterns_to_try, 1):
                try:
                    print(f"   ðŸ” Try {i}/{len(patterns_to_try)}: {pattern}")
                    config_path = hf_hub_download(
                        repo_id=finetune_repo,
                        filename=pattern
                    )
                    print(f"   âœ… Config found and downloaded: {pattern}")
                    return config_path
                except Exception as e:
                    print(f"   âŒ Not found: {pattern}")
                    continue
            
            print(f"   âš ï¸  No versioned config found for {version_prefix}")
        
        # FIXED: Fallback 1 - Try generic base_model_config.json from HF repo FIRST
        try:
            print(f"   ðŸ” Trying base_model_config.json from {finetune_repo}...")
            config_path = hf_hub_download(
                repo_id=finetune_repo,
                filename="base_model_config.json"
            )
            print(f"   âœ… Config downloaded from HF repo: {config_path}")
            return config_path
        except Exception as e:
            print(f"   âš ï¸  No base_model_config.json in finetune repo: {e}")
        
        # Fallback 2: Check for local base_model_config.json (backward compat)
        local_config = os.path.abspath("./base_model_config.json")
        if os.path.exists(local_config):
            print(f"   âš ï¸  Using local config as fallback: {local_config}")
            return local_config
        
        # Fallback 3: Return None to signal using base_repo config
        print(f"   âš ï¸  No config detected, will use base_repo config")
        return None
    
    def _fetch_prompts_for_repo(self, repo_id: str, checkpoint: str | None = None):
        from huggingface_hub import hf_hub_download
        import json
        try:
            ppath = hf_hub_download(repo_id=repo_id, filename="prompts.json")
            with open(ppath, "r", encoding="utf-8") as f:
                data = json.load(f)
            # Optional: stamp what we loaded
            if checkpoint and isinstance(data, dict):
                data.setdefault("finetune_repo", repo_id)
                data.setdefault("checkpoint", checkpoint)
            return data
        except Exception:
            return None

    def get_model(self, model_spec="standard"):
        """Get model from cache or load it"""
        with self.model_lock:
            # Cache key
            if model_spec == "standard":
                cache_key = "standard_saos"
            else:
                base_repo_key = model_spec.get('base_repo', 'stabilityai/stable-audio-open-small').replace('/', '_')
                repo_key = model_spec['repo'].replace('/', '_')
                ckpt_key = model_spec['checkpoint'].replace('/', '_').replace('.', '_')
                cache_key = f"finetune_{repo_key}_{ckpt_key}"

            # Cached?
            if cache_key in self.model_cache:
                print(f"ðŸŽ¯ Using cached model: {cache_key}")
                self._update_usage(cache_key)
                return self.model_cache[cache_key]

            # Evict LRU if needed
            self._evict_lru_model()
            print(f"ðŸ”„ Loading model: {cache_key}")

            try:
                if model_spec == "standard":
                    model_data = self._load_standard_model()
                else:
                    model_data = self._load_finetune_model(model_spec)

                self.model_cache[cache_key] = model_data
                self._update_usage(cache_key)

                print(f"âœ… Model loaded and cached: {cache_key}")
                print(f"ðŸ“Š Cache status: {len(self.model_cache)}/{self.max_models} models loaded")
                return model_data

            except Exception as e:
                print(f"âŒ Failed to load model {cache_key}: {e}")
                raise
    
    def _load_standard_model(self):
        """Load standard SAOS model (post-adversarial training)"""
        import os, torch
        from huggingface_hub import login

        # HF auth if provided
        hf_token = os.getenv('HF_TOKEN')
        if hf_token:
            login(token=hf_token)

        model, config = get_pretrained_model("stabilityai/stable-audio-open-small")

        device = "cuda" if torch.cuda.is_available() else "cpu"
        model = model.to(device)
        if device == "cuda":
            model = model.half()
        model.eval()

        obj = getattr(model, "diffusion_objective", None)
        print(f"âœ… Standard SAOS ready | objective: {obj} | device: {device}")
        print(f"   Sample rate: {config.get('sample_rate')}")
        print(f"   Sample size: {config.get('sample_size')}")

        return {
            "model": model,
            "config": config,
            "device": device,
            "type": "standard",
            "source": "stabilityai/stable-audio-open-small",
            "base_repo": "stabilityai/stable-audio-open-small",
        }
    
    def _load_finetune_model(self, model_spec):
        """
        Load a finetune with smart config detection and proper EMA handling:
        - Auto-detects versioned configs (acid_v1, acid_v2, etc.)
        - Prioritizes HF repo config over local config
        - Handles EMA weights properly (diffusion_ema.ema_model.* â†’ diffusion.model.*)
        - Loads base weights first, then overlays finetune (excluding pretransform)
        
        FIXED: Added EMA weight remapping from test script
        """
        import os, json
        import torch
        from huggingface_hub import hf_hub_download, login

        repo = model_spec["repo"]
        checkpoint_name = model_spec["checkpoint"]
        base_repo = model_spec.get("base_repo", "stabilityai/stable-audio-open-small")

        hf_token = os.getenv("HF_TOKEN")
        if hf_token:
            login(token=hf_token)

        try:
            # Smart config detection (with fixed priority)
            config_path = self._detect_config_from_checkpoint(checkpoint_name, repo)
            
            if config_path is None:
                # Fallback: use base_repo config
                print(f"ðŸ“¥ Downloading config from base_repo: {base_repo}")
                config_path = hf_hub_download(repo_id=base_repo, filename="base_model_config.json")

            print(f"ðŸ“¥ Downloading base_model.ckpt from {base_repo}")
            base_ckpt_path = hf_hub_download(repo_id=base_repo, filename="base_model.ckpt")

            print(f"ðŸ“¥ Downloading finetune checkpoint {checkpoint_name} from {repo}")
            ft_ckpt_path = hf_hub_download(repo_id=repo, filename=checkpoint_name)

            # Build model from config
            with open(config_path, "r") as f:
                config = json.load(f)

            print("ðŸ”§ Creating model from configâ€¦")
            print(f"   Config path: {config_path}")
            print(f"   Config keys: {list(config.keys())}")
            model = create_model_from_config(config)

            # Load base weights using copy_state_dict (like test script)
            print("ðŸŽ¯ Loading base_model.ckpt using copy_state_dict")
            base_sd = load_ckpt_state_dict(base_ckpt_path)
            copy_state_dict(model, base_sd)
            print(f"   âœ… Base weights loaded")

            # Load finetune checkpoint
            print("ðŸŽ¯ Loading finetune checkpoint...")
            ckpt = torch.load(ft_ckpt_path, map_location="cpu")
            state_dict = ckpt.get("state_dict", ckpt)
            
            # FIXED: Handle EMA weights (from test script)
            ema_keys = [k for k in state_dict.keys() if k.startswith("diffusion_ema.ema_model.")]
            
            if ema_keys:
                print(f"   ðŸ” Found EMA weights ({len(ema_keys)} keys)")
                # Create new state dict with EMA weights, stripping the prefix
                ema_state = {}
                for key in ema_keys:
                    # Strip "diffusion_ema.ema_model." prefix to get "diffusion.model.*"
                    new_key = key.replace("diffusion_ema.ema_model.", "diffusion.model.")
                    ema_state[new_key] = state_dict[key]
                
                # Also include pretransform weights from base (not EMA'd)
                print(f"   ðŸ” Keeping pretransform weights from base model")
                for key in state_dict.keys():
                    if key.startswith("diffusion.pretransform"):
                        ema_state[key] = state_dict[key]
                
                # Use EMA state
                print(f"   âœ… Using {len(ema_state)} EMA-remapped weights")
                copy_state_dict(model, ema_state)
            else:
                print(f"   â„¹ï¸  No EMA weights found, using regular state_dict")
                # Filter out pretransform to preserve base model's pretransform
                filtered_state = {k: v for k, v in state_dict.items() 
                                if not k.startswith("pretransform.")}
                print(f"   âœ… Using {len(filtered_state)} regular weights (excluding pretransform)")
                copy_state_dict(model, filtered_state)

            # Device, dtype, eval()
            device = "cuda" if torch.cuda.is_available() else "cpu"
            model = model.to(device)
            if device == "cuda":
                model = model.half()
            model.eval()

            obj = getattr(model, "diffusion_objective", None)
            print(f"âœ… Finetune model ready | objective: {obj} | device: {device}")
            print(f"   Config: {os.path.basename(config_path)}")
            print(f"   Checkpoint: {checkpoint_name}")
            print(f"   Sample rate: {config.get('sample_rate')}")
            print(f"   Sample size: {config.get('sample_size')}")

            prompts = self._fetch_prompts_for_repo(repo_id=repo, checkpoint=checkpoint_name)

            return {
                "model": model,
                "config": config,
                "device": device,
                "type": "finetune",
                "source": f"{repo}/{checkpoint_name}",
                "config_source": config_path,
                "base_source": base_repo,
                "repo": repo,
                "checkpoint": checkpoint_name,
                "base_repo": base_repo,
                "prompts": prompts,  # <-- add this
            }

        except Exception as e:
            print(f"âŒ Error loading finetune: {str(e)}")
            import traceback
            traceback.print_exc()
            raise
    
    def list_loaded_models(self):
        """Get info about currently loaded models"""
        with self.model_lock:
            return {
                "loaded_models": list(self.model_cache.keys()),
                "usage_order": self.model_usage_order.copy(),
                "cache_utilization": f"{len(self.model_cache)}/{self.max_models}"
            }
    
    def clear_cache(self):
        """Clear all cached models"""
        with self.model_lock:
            print("ðŸ§¹ Clearing all cached models...")
            self.model_cache.clear()
            self.model_usage_order.clear()
            with self.resource_cleanup():
                pass
            print("âœ… Model cache cleared")

# Global model manager instance
model_manager = ModelManager(max_models=2)

def load_model(model_type="standard", finetune_repo=None, finetune_checkpoint=None, base_repo=None):
    """
    Load model using the enhanced model manager with auto-config detection
    
    Args:
        model_type: "standard" or "finetune"
        finetune_repo: HF repo for finetune (e.g., "thepatch/jerry_grunge")
        finetune_checkpoint: Checkpoint filename (e.g., "jerry_encoded_bs128_HARD_epoch=19-step=60.ckpt")
        base_repo: Base repo for base weights (optional, defaults to "stabilityai/stable-audio-open-small")
    
    Returns:
        Tuple of (model, config, device)
    
    Note:
        Config is now auto-detected with proper priority:
        1. Versioned configs from checkpoint name (acid_v1_*, acid_v2_*, etc.)
        2. base_model_config.json from HuggingFace repo
        3. Local base_model_config.json (backward compat)
        4. Base repo config
        
        Also handles EMA weights properly (diffusion_ema.ema_model.* â†’ diffusion.model.*)
    """
    if model_type == "standard":
        model_spec = "standard"
    elif model_type == "finetune":
        if not finetune_repo or not finetune_checkpoint:
            raise ValueError("finetune_repo and finetune_checkpoint required for finetune models")
        model_spec = {
            "type": "finetune",
            "repo": finetune_repo,
            "checkpoint": finetune_checkpoint,
            "base_repo": base_repo or "stabilityai/stable-audio-open-small"
        }
    else:
        raise ValueError("model_type must be 'standard' or 'finetune'")
    
    model_data = model_manager.get_model(model_spec)
    return model_data["model"], model_data["config"], model_data["device"]

# Test function
def test_versioned_checkpoints():
    """Test loading different versioned checkpoints"""
    print("ðŸ§ª Testing versioned checkpoint loading...\n")
    
    test_cases = [
        {
            "name": "jerry_grunge (NEW: Fixed config + EMA)",
            "repo": "thepatch/jerry_grunge",
            "checkpoint": "jerry_encoded_bs128_HARD_epoch=19-step=60.ckpt"
        },
        {
            "name": "acid_v1",
            "repo": "S3Sound/acid_saos",
            "checkpoint": "acid_v1_saos_e21_s33000.ckpt"
        },
        {
            "name": "acid_v4",
            "repo": "S3Sound/acid_saos",
            "checkpoint": "acid_v4_saos_e22_s16896.ckpt"
        }
    ]
    
    for test in test_cases:
        try:
            print(f"\n{'='*60}")
            print(f"Testing: {test['name']}")
            print(f"{'='*60}")
            
            model, config, device = load_model(
                model_type="finetune",
                finetune_repo=test["repo"],
                finetune_checkpoint=test["checkpoint"]
            )
            
            print(f"âœ… {test['name']} loaded successfully!")
            print(f"   Device: {device}")
            print(f"   Sample rate: {config['sample_rate']}")
            print(f"   Sample size: {config['sample_size']}")
            print(f"   Max duration: {config['sample_size'] / config['sample_rate']:.1f}s")
            
        except Exception as e:
            print(f"âŒ {test['name']} failed: {e}")
    
    # Show cache status
    print(f"\n{'='*60}")
    print("Final Cache Status:")
    print(f"{'='*60}")
    cache_info = model_manager.list_loaded_models()
    print(f"Loaded models: {cache_info['loaded_models']}")
    print(f"Cache utilization: {cache_info['cache_utilization']}")

if __name__ == "__main__":
    # Run test
    test_versioned_checkpoints()